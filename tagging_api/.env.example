# Wiki Tagging Service Configuration

# ============================================================================
# SERVICE CONFIGURATION
# ============================================================================
SERVICE_NAME=wiki-tagging-service
VERSION=1.0.0
API_TOKEN=your-secret-token-change-this-in-production
PORT=8002
HOST=0.0.0.0
WORKERS=1
LOG_LEVEL=INFO

# ============================================================================
# REDIS CONFIGURATION
# ============================================================================
REDIS_URL=redis://localhost:6379/0
REDIS_QUEUE_NAME=tagging

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Model name from HuggingFace
# Options:
#   - google/gemma-2-2b-it (Recommended starter - 2B params, ~4GB VRAM)
#   - microsoft/Phi-3-mini-4k-instruct (3.8B params, ~5GB VRAM)
#   - google/gemma-2-9b-it (Best quality - 9B params, ~10GB VRAM)
#   - meta-llama/Meta-Llama-3.1-8B-Instruct (8B params, ~10GB VRAM)
MODEL_NAME=google/gemma-2-2b-it

# Model type (transformers, llama-cpp, vllm)
MODEL_TYPE=transformers

# Device (cuda, cpu, mps)
DEVICE=cuda

# Device mapping strategy
DEVICE_MAP=auto

# Cache directory for downloaded models
CACHE_DIR=./models

# ============================================================================
# QUANTIZATION
# ============================================================================

# Quantization mode (none, 4bit, 8bit)
QUANTIZATION=4bit

# Load in 4-bit (recommended for GPU with limited VRAM)
LOAD_IN_4BIT=true

# Load in 8-bit (higher quality, more VRAM)
LOAD_IN_8BIT=false

# ============================================================================
# GENERATION PARAMETERS
# ============================================================================

# Temperature (0.0 = deterministic, 1.0 = creative)
# Lower values recommended for consistent tagging
TEMPERATURE=0.3

# Top-p sampling (nucleus sampling)
TOP_P=0.9

# Top-k sampling
TOP_K=50

# Maximum tokens to generate
MAX_NEW_TOKENS=1000

# Repetition penalty (1.0 = no penalty)
REPETITION_PENALTY=1.1

# ============================================================================
# PROCESSING LIMITS
# ============================================================================

# Maximum input tokens to process
MAX_INPUT_TOKENS=6000

# Maximum tags per page
MAX_TAGS_PER_PAGE=10

# Minimum confidence threshold (0.0 to 1.0)
MIN_CONFIDENCE=0.0

# Batch size (keep at 1 to manage GPU memory)
BATCH_SIZE=1

# ============================================================================
# PROMPT CONFIGURATION
# ============================================================================

# Default prompt template (detailed, quick, technical, general)
DEFAULT_PROMPT_TEMPLATE=detailed

# Directory containing prompt templates
PROMPT_TEMPLATES_DIR=./prompts

# ============================================================================
# CACHING
# ============================================================================

# Enable tag embedding cache for faster matching
ENABLE_TAG_CACHE=true

# Cache TTL in seconds (86400 = 24 hours)
CACHE_TTL_SECONDS=86400

# ============================================================================
# TIMEOUTS AND RETRIES
# ============================================================================

# Inference timeout in seconds
INFERENCE_TIMEOUT_SECONDS=30

# Maximum retry attempts for failed jobs
MAX_RETRIES=3

# Delay between retries in seconds
RETRY_DELAY_SECONDS=5

# ============================================================================
# EMBEDDING MODEL FOR TAG MATCHING
# ============================================================================

# Sentence transformer model for semantic similarity
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Similarity threshold for matching existing tags (0.0 to 1.0)
# 0.75 means 75% similar = match
SIMILARITY_THRESHOLD=0.75
